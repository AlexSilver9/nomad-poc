- Clusters Cloud Native:
    - Dev/Smoke: Hier lÃ¤uft alles, und es darf auch mal rauchen, Cluster Development
    - Stageing/Release/UAT: Hier lÃ¤uft alles, Kunden kÃ¶nnen gegen Release Candidates testen
    - Prod: Hier lÃ¤uft alles

- Client node introduction token? -> YES
    - https://developer.hashicorp.com/nomad/docs/deploy/clusters/connect-nodes#use-client-node-introduction-tokens
    - Node darf nur bestimmten NodePool joinen
    - und/oder Node muss bestimmten Namen haben um joinen zu dÃ¼rfen
    - und/oder TTL

- How to connect to my host network when using Docker Desktop (Windows and MacOS)?
    - https://developer.hashicorp.com/nomad/docs/faq#q-how-to-connect-to-my-host-network-when-using-docker-desktop-windows-and-macos

- Nomad als root fÃ¼r Docker Plugin 'resources.cores' -> NO
    - Nomad as root or in docker group to access Docker Unix socket
    - resources.cores geht (CPU Isolation / Numa Scheduling) geht nur mit Nomad als root, ebenso nicht-Docker Driver Tasks
    - https://developer.hashicorp.com/nomad/docs/deploy/task-driver/docker#client-requirements
    - Ohne root -> stÃ¤ndige WARN im Log

- Nomad als:
    - Binary mit manuellem Control und Version Control
    - Binary mit systemd Control und manuellem Version Control
    - Linux Package mit systemd Control und Linux Version Control -> YES

- Nomad Enterprise Features (Contact Sales):
    - https://developer.hashicorp.com/nomad/docs/enterprise
    - https://www.hashicorp.com/de/pricing?tab=nomad
    - â“ Time-based task execution
    - âŒ Multi-cluster deployment (Multi region)
    - Node pool governance
    - âŒ Enhanced read scalability (Raft non-voters for reads scalability / scheduling throughput)
    - âŒ Non-uniform memory access (NUMA) support
    - â“ Dynamic Application Sizing (resource consumption of applications using resource sizing recommendations)
    - â“ Audit logging
    - â“ Resource quotas (restrict the aggregate resource usage of namespaces/regions)
    - â“ Sentinel policies (fine-grained policies on top of ACL)
    - âŒ Automated backups (snapshots of the state of the Nomad server)
    - â“ Automated upgrades (on new server version promote new servver & degrade old servers once new servers are >= 50% ) -> Check manual upgrades 
    - âŒ Redundancy zones (deploy a non-voting server as a hot standby server on a per availability zone basis)
    - â“ Long term support releases (receive critical fixes and security patches between LTS releases, and hardened upgrade paths to the next LTS release)
    - âŒ Multiple Vault namespaces
    - âŒ Consul namespace support
    - âŒ Multiple Vault cluster support
    - âŒ Multiple Consul cluster support

------------------------------------------------------------------------------------------------

# AuflÃ¶sung des statischen Load Balancing

- Aktuell ist ein vorgeschalteter AWS ALB im Einsatz, der alles strikt an einen Nginx LB Docker container auf einer festen Instanz routet
- Der Nginx routet je nach Port den Traffic an andere Container, die jeweils ebenso als Docker container auf festen Instanzen laufen

Ist-Zustand:
```text
Internet
  â”‚
  â–¼
AWS ALB
  â”‚  (statisch)
  â–¼
EC2 A
 â””â”€ nginx (Docker)
      â”œâ”€ :8081 â†’ (Docker) Service A (auf EC2 B)
      â”œâ”€ :8082 â†’ (Docker) Service B (auf EC2 C)
      â””â”€ :8083 â†’ (Docker) Service C (auf EC2 D)
```

- Nginx als LB auf indizierter Instanz = **Single Point of Failure**
- Container Anti-Orchestration

Ziel-Zustand:
```text
Internet
  â”‚
  â–¼
AWS ALB
  â”‚  (dynamisch)
  â–¼
Nomad Allocations (beliebige EC2s)
```

- Nomad entscheidet Placement â†’ keine festen EC2-Zuordnungen, keine festen Ports
- Der ALB hat kein dynamisches Wissen â†’ muss auf stabile, langlebige Ziele routen
- Services sind reine Edge-Services â†’ kein interner Service-zu-Service Traffic nÃ¶tig
- **Traffic darf nur kontrolliert umgeschwenkt werden** (Sessions liegen lokal)
- Hot-Standby & Schwenken â†’ LB muss Umschalten erlauben, ohne ALB-Rekonfiguration

- âŒ Das schlieÃŸt ALB-direct-to-Allocations aus.
- âŒ Das schlieÃŸt statische nginx-Ports aus

## Optimale Architektur

```text
Internet
   â”‚
   â–¼
AWS ALB (statisch)
   â”‚
   â–¼
Consul Ingress Gateway ("Envoy" als Nomad Job, HA)
   â”‚
   â–¼
Nomad Services (dynamisch, beliebige EC2s)
```

Der ALB:
- sieht nur ein stabiles Ziel, keine dynamischen Ports keine Service-Details
- alles Dynamische passiert hinter dem ALB.
- keine AWS IAM Management fÃ¼r dynamische Regeln im ALB 


### Consul Ingress vs. Nginx

| Anforderung                             | Nginx            | Consul Ingress  |
|-----------------------------------------|------------------|-----------------|
| Statischer ALB	                        | âœ…	              | âœ…             |
| Nomad Placement frei                    | âš ï¸	             | âœ…             |
| Dynamische Backends                     | âš ï¸ (DNS Reloads) | âœ…             |
| Kontrolliertes Umschwenken              | âŒ               | âœ…             |
| Hot-Standby                             | âŒ               | âœ…             |
| Health-gesteuertes Routing              | âš ï¸               | âœ…             |
| Kein Singe Point of Failure             | âŒ               | âœ…             |
| Zero-Downtime Drains                    | âŒ               | âœ…             |

### Ingress Gateway als Nomad Job

#### Was ist ein Ingress?

Ein Ingress ist eine Config Resource die definiert:
 - Welche externen Anfragen wohin im Cluster weitergeleitet werden. Beispiele:
    - /api/users â†’ user-service
    - /api/payments â†’ payment-service
- Optionales Routing nach Hostnamen, z.â€¯B.:
    - users.example.com â†’ user-service
- TLS/SSL-Termination, sodass der Service selbst kein Zertifikat verwalten muss
- Load Balancing, typischerweise auf Container-Ebene (mehrere Instanzen eines Services)

**Ingress ist nicht der Load Balancer selbst, sondern die Konfiguration, die das Routing festlegt.**

### Ingress Gateway Setup:
- 3 Replikas (oder auf allen Nodes)
- Statischer Port (z. B. 8080)
- Registriert sich einmal beim ALB

AWS ALB:

```text
Target Group â†’ ingress-gateway:8080
```

### Services als Connect-enabled Nomad Jobs

```hcl
service {
  name = "web"
  port = "http"

  connect {
    sidecar_service {}
  }
}
```

- â¡ï¸ Keine Port-Kollisionen
- â¡ï¸ Kein Wissen Ã¼ber IPs
- â¡ï¸ Kein manuelles Routing

### Routing (ersetzt Port-basierte nginx-Logik)

```hcl
Kind = "ingress-gateway"
Name = "ingress"

Listeners = [
  {
    Port = 8080
    Protocol = "http"
    Services = [
      {
        Name  = "service-a"
        Hosts = ["a.example.com"]
      },
      {
        Name  = "service-b"
        Hosts = ["b.example.com"]
      }
    ]
  }
]
```

### Hot-Standby & Kernel-Upgrade

Ablauf bei EC2 Kernel Upgrade:

1. Drain Node
    - `nomad node drain <node-id> -enable`
2. Nomad startet neue Allocations
    - Auf **anderen EC2s**
    - Connect Sidecars melden sich bei Consul
    - Ingress Gateway sieht neue Backends
3.  Traffic Shift (automatisch!)
    - Ingress routet nur auf healthy Instanzen
    - Alte Allocations bekommen kein neuen Traffic
    - Sessions laufen kontrolliert aus
4. Node wird leer
    - Alte Allocations stoppen
    - EC2 kann sicher rebootet / gepatcht werden
5. Node kommt zurÃ¼ck
    - `nomad node drain <node-id> -disable`


- âœ… Kein ALB-Eingriff
- âœ… Keine reale Downtime (Session Rebuild ausgenommen)
- âœ… Kein Traffic-Verlust

### Sessions

Da Sessions lokal liegen:
  - Ingress Gateway:
    - â“ unterstÃ¼tzt Consistent Hashing
    - â“ oder Source-IP Affinity

- â¡ï¸ Sessions bleiben stabil
- â¡ï¸ Umschwenken nur bei Health-Failure

### Warum das â€optimalâ€œ ist

- Der ALB bleibt dumm & stabil
- Nomad bekommt volle Freiheit
- Consul Ã¼bernimmt dynamische Wahrheit
- Ingress ist austauschbar & skalierbar
- Kernel-Upgrades sind kontrollierte Operationen

# Legacy Nginx

Wichtigste Design-Entscheidung

â“ Nginx weiterhin als Ingress behalten â€“ oder abschaffen?

- Consul ist kein Proxy und kein Reverse-Proxy

Konkreter Vergleich: nginx vs Consul Ingress Gateway

| Feature | nginx | Consul Ingress |
|---------|-------|----------------|
| Fester Einstiegspunkt | âœ… | âœ… |
| Dynamische Backends | âš ï¸ (DNS/Reload) | âœ… |
| Healthchecks | âš ï¸ | âœ… |
| Load Balancing | âœ… | âœ… |
| Zero-Downtime Scaling | âŒ | âœ… |
| mTLS | âŒ | âœ… |
| Nomad-native | âŒ | âœ… |
| Statische Config | âŒ | âŒ |


# URL REWRITE with TRAEFIK
```text
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              AWS ALB                â”‚
                    â”‚  Target Group: all instances:8081   â”‚
                    â”‚          TLS Termination            |
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                       â–¼                       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   nomad1      â”‚       â”‚   nomad2      â”‚       â”‚   nomad3      â”‚  <-- Nomad Cluster
   â”‚---------------â”‚       â”‚---------------â”‚       â”‚---------------|
   â”‚ Traefik:8081  â”‚       â”‚ Traefik:8081  â”‚       â”‚ Traefik:8081  â”‚  <-- URL Rewrite
   â”‚      â†“        â”‚       â”‚      â†“        â”‚       â”‚      â†“        â”‚
   â”‚ Envoy:8080    â”‚       â”‚ Envoy:8080    â”‚       â”‚ Envoy:8080    â”‚  <-- Ingress
   â”‚      â†“        â”‚       â”‚      â†“        â”‚       â”‚      â†“        â”‚
   â”‚   Sidecar     â”‚       â”‚   Sidecar     â”‚       â”‚   Sidecar     â”‚  <-- Local Proxy
   â”‚      â†“        â”‚       â”‚      â†“        â”‚       â”‚      â†“        â”‚
   â”‚   Service     â”‚       â”‚   Service     â”‚       â”‚   Service     â”‚  <-- Application
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## Files involved in routing

| File	                      | Responsibility                                      |
|-----------------------------|-----------------------------------------------------|
| traefik-rewrite.hcl	        | URL regex rewrite, forwards to Envoy                |
| ingress-gateway.hcl	        | Nomad job for Envoy ingress on :8080 (includes host â†’ service mapping) |
| business-service-router.hcl | Consul config: path-based routing (/swdlgwapi vs /) |


## Traefik vs nginx for URL Rewriting

| Aspect | Traefik | nginx |
|--------|---------|-------|
| Rewrite type | `redirectRegex` (302 redirect, client follows) | `rewrite ... break` (internal, transparent) |
| Regex scope | Matches full URL (`https?://[^/]+/...`) | Matches path only (`^/download/(.*)$`) |
| Capture syntax | `${1}`, `${2}` | `$1`, `$2` |
| Config format | YAML (dynamic.yaml) | nginx.conf |
| Client behavior | Must follow redirect (`curl -L`) | Transparent, no redirect |
| Query string | Proper `?` delimiter via redirect | Proper `?` delimiter via internal rewrite |
| Listen port | 8081 | 8081 |
| Listen port | HTTP on 443 allowed | HTTPs on 443 expected |


ğŸ‘‰ âš ï¸ âœ… âŒ â“â¡ï¸


